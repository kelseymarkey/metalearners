{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.grf import RegressionForest\n",
    "from sklearn import clone\n",
    "import os, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class t_learner:\n",
    "\n",
    "    '''\n",
    "    From Rosenberg Slides:\n",
    "        µ^0(x) = E[Y (0) | X = x]\n",
    "        µ^1(x) = E[Y (1) | X = x]\n",
    "        τˆT (x) = µˆ1(x) − µˆ0(x)\n",
    "\n",
    "    authors: Alene Rhea and Tamar Novetsky, April 1 2021\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mu0_base, mu1_base):\n",
    "        # Make copies of initialized base learners\n",
    "        self.mu0_base = clone(mu0_base, safe=False)\n",
    "        self.mu1_base = clone(mu1_base, safe=False)\n",
    "\n",
    "    def fit(self, X, W, y):\n",
    "        # Call fit methods on each base learner\n",
    "        self.mu0_base.fit(X[W==0], y[W==0])\n",
    "        self.mu1_base.fit(X[W==1], y[W==1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y1_preds = self.mu1_base.predict(X)\n",
    "        y0_preds = self.mu0_base.predict(X)\n",
    "        tau_preds = y1_preds - y0_preds\n",
    "        return tau_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s_learner:\n",
    "\n",
    "    '''\n",
    "    From Rosenberg Slides:\n",
    "        µ^(x,w) = E[Y | X = x, W = w]\n",
    "        τˆS (x) = µˆ(x,1) − µˆ(x,0)\n",
    "\n",
    "    authors: Kelsey Markey and Lauren D'Arinzo, April 4 2021\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mu_base):\n",
    "        # Make copies of initialized base learner\n",
    "        self.mu_base = clone(mu_base, safe=False)\n",
    "\n",
    "    def fit(self, X_W, y):\n",
    "        # Call fit method\n",
    "        self.mu_base.fit(X_W, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_W_1 = X.copy(deep=True)\n",
    "        X_W_1[\"W\"] = pd.Series(np.ones(len(X_W_1)))\n",
    "        y1_preds = self.mu_base.predict(X_W_1)\n",
    "\n",
    "        X_W_0 = X.copy(deep=True)\n",
    "        X_W_0[\"W\"] = pd.Series(np.zeros(len(X_W_0)))\n",
    "        y0_preds = self.mu_base.predict(X_W_0)\n",
    "\n",
    "        tau_preds = y1_preds - y0_preds\n",
    "        return tau_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x_learner:\n",
    "\n",
    "    '''\n",
    "    From Rosenberg Slides:\n",
    "        µ^0(x) = E[Y (0) | X = x]\n",
    "        µ^1(x) = E[Y (1) | X = x]\n",
    "        τˆx (x) = g(x)t_0ˆ(x) − (1-g(x))t_1ˆ(x)\n",
    "\n",
    "    authors: Kelsey Markey and Lauren D'Arinzo, April 4 2021\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mu0_base, mu1_base, tau0_base, tau1_base):\n",
    "        # Make copies of initialized base learner\n",
    "        self.mu0_base = clone(mu0_base, safe=False)\n",
    "        self.mu1_base = clone(mu1_base, safe=False)\n",
    "        self.tau0_base = clone(tau0_base, safe=False)\n",
    "        self.tau1_base = clone(tau1_base, safe=False)\n",
    "\n",
    "    def fit(self, X, W, y, fit_g):\n",
    "        '''\n",
    "        fit_g : boolean indicator if g should be fit to training data. if false, g must be passed explicitlly to x_learner.predict()\n",
    "        '''\n",
    "        # Call fit method\n",
    "        self.mu0_base.fit(X[W==0], y[W==0])\n",
    "        self.mu1_base.fit(X[W==1], y[W==1])\n",
    "    \n",
    "        #Impute y0 for treated group using mu0\n",
    "        y0_treat=self.mu0_base.predict(X[W==1]).flatten()\n",
    "        imputed_TE_treatment = y[W==1] - y0_treat\n",
    "\n",
    "        #Impute y1 for control group using mu1\n",
    "        y1_control=self.mu1_base.predict(X[W==0]).flatten()\n",
    "        imputed_TE_control = y1_control - y[W==0] \n",
    "\n",
    "        #Fit tau0: CATE estimate fit to the control group\n",
    "        self.tau0_base.fit(X[W==0], imputed_TE_control)\n",
    "\n",
    "        #Fit tau1: CATE estimate fit to the treatment group\n",
    "        self.tau1_base.fit(X[W==1], imputed_TE_treatment)\n",
    "\n",
    "        if fit_g:\n",
    "            print('X Learner with g fitted')\n",
    "            #predict propensities from empirical data\n",
    "            self.g_fit = LogisticRegression(fit_intercept=True, max_iter=2000).fit(\n",
    "            X=X, y=W)\n",
    "        \n",
    "        else:\n",
    "            print('X Learner with true propensities')\n",
    "            self.g_fit = None\n",
    "\n",
    "\n",
    "    def predict(self, X, g):\n",
    "        '''\n",
    "        g : weight vector that should be length of the test set. can be passed as None if g was fit to data\n",
    "        '''\n",
    "        tau0_preds = self.tau0_base.predict(X).flatten()\n",
    "        tau1_preds = self.tau1_base.predict(X).flatten()\n",
    "    \n",
    "    \n",
    "        if self.g_fit == None:\n",
    "            tau_preds = (g.T * tau0_preds) + ((1-g).T*tau1_preds)\n",
    "        else:\n",
    "            g_preds = self.g_fit.predict_proba(X)[:, 1]\n",
    "            tau_preds = (g_preds.T * tau0_preds) + ((1-g_preds).T*tau1_preds)\n",
    "\n",
    "        # idea: allow g to be either be a vector or function?\n",
    "        # if function: think about sklearn inputs (.predict) vs lamba functions (g(x))\n",
    "        # if g_type == 'Function':\n",
    "        #    g_preds = g(X)\n",
    "        #    tau_preds = (g_preds.T * tau0_preds) + ((1-g_preds).T*tau1_preds)\n",
    "\n",
    "        return tau_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_get_mse_t(train, test, mu0_base, mu1_base):\n",
    "    '''\n",
    "    mu0_base: base learner that has already been initialized\n",
    "    mu1_base: base learner that has already been initialized\n",
    "    '''\n",
    "    #data preprocessing\n",
    "    X_train = train.drop(columns=['treatment', 'Y', 'tau', 'pscore'])\n",
    "    y_train = train['Y']\n",
    "    W_train = train['treatment']\n",
    "    X_test = test.drop(columns=['treatment', 'Y', 'tau', 'pscore'])\n",
    "\n",
    "    #initialize metalearner\n",
    "    T = t_learner(mu0_base=mu0_base, mu1_base=mu1_base)\n",
    "    T.fit(X=X_train, W=W_train, y=y_train)\n",
    "    \n",
    "    # Predict test-set CATEs\n",
    "    tau_preds = T.predict(X=X_test)\n",
    "\n",
    "    # Calculate MSE on test set\n",
    "    mse = np.mean((tau_preds - test.tau)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_get_mse_s(train, test, mu_base):\n",
    "    '''\n",
    "    mu: base learner that has already been initialized\n",
    "    '''\n",
    "    #data preprocessing\n",
    "    X_train = train.drop(columns=['treatment', 'Y', 'tau', 'pscore'])\n",
    "    y_train = train['Y']\n",
    "    W_train = train['treatment']\n",
    "    X_test = test.drop(columns=['treatment', 'Y', 'tau', 'pscore'])\n",
    "\n",
    "    #initialize metalearner\n",
    "    S = s_learner(mu_base=mu_base)\n",
    "    \n",
    "    #fit S-learner\n",
    "    X_W = pd.concat([X_train, W_train], axis=1)\n",
    "    S.fit(X_W=X_W, y=y_train)\n",
    "    \n",
    "    # Predict test-set CATEs\n",
    "    tau_preds = S.predict(X=X_test)\n",
    "\n",
    "    # Calculate MSE on test set\n",
    "    mse = np.mean((tau_preds - test.tau)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set root directory\n",
    "    # base_repo_dir = pathlib.Path(os.path.realpath(__file__)).parents[0]\n",
    "    base_repo_dir = os.getcwd()\n",
    "    \n",
    "    #read in tuned hyperparameter files\n",
    "    rf_t = json.load(open(base_repo_dir + '/configurations/hyperparameters/' + 'rf_t.json'))\n",
    "    rf_s = json.load(open(base_repo_dir + '/configurations/hyperparameters/' + 'rf_s.json'))\n",
    "    \n",
    "    #rf_params = {'T': rf_T, 'S': rf_S, 'X': rf_X}\n",
    "    rf_params = {'T': rf_t, 'S': rf_s}\n",
    "    \n",
    "    #read in base learner model types for each metalearner\n",
    "    meta_base_dict = json.load(open(base_repo_dir + '/configurations/base_learners/' + 'base_learners.json'))\n",
    "    \n",
    "    for train_size in [5000, 10000]: #, 20000, 100000, 300000\n",
    "        print('---------------------------')\n",
    "        print('Training set size:', train_size)\n",
    "        for sim in ['simA', 'simB']:\n",
    "            print('---------------------------')\n",
    "            print('Starting '+ sim)\n",
    "            for i in range(3):\n",
    "                print('')\n",
    "                train = pd.read_parquet(base_repo_dir + '/data/' + str(train_size) + '/' + sim + \n",
    "                                        '/samp'+str(i+1)+'_train.parquet')\n",
    "                test = pd.read_parquet(base_repo_dir + '/data/' + str(train_size) + '/' + sim +  \n",
    "                                       '/samp'+str(i+1)+'_test.parquet')\n",
    "                for metalearner in meta_base_dict.keys():\n",
    "\n",
    "                    if metalearner == 'T':\n",
    "                    # below logic needs to be generalized for other metalearners\n",
    "                        for base_learner_dict in meta_base_dict[metalearner]:\n",
    "                            print(sim+'/' +'sample_'+str(i+1)+'/'+metalearner+'-learner:')\n",
    "                            if base_learner_dict['mu_0'] == 'rf':\n",
    "                                mu0_hyperparams = rf_params[metalearner]['mu_0'][sim]\n",
    "                                mu0_base = RegressionForest(honest=True, random_state=42, **mu0_hyperparams)\n",
    "                            if base_learner_dict['mu_1'] == 'rf':\n",
    "                                mu1_hyperparams = rf_params[metalearner]['mu_1'][sim]\n",
    "                                mu1_base = RegressionForest(honest=True, random_state=42, **mu1_hyperparams)\n",
    "                            mse = fit_get_mse_t(train, test, mu0_base, mu1_base)\n",
    "                            print('     MSE=', mse)\n",
    "\n",
    "                    if metalearner == 'S':\n",
    "                        for base_learner_dict in meta_base_dict[metalearner]:\n",
    "                            print(sim+'/' +'sample_'+str(i+1)+'/'+ metalearner+'-learner:')\n",
    "                            if base_learner_dict['mu'] == 'rf':\n",
    "                                mu_hyperparams = rf_params[metalearner]['mu'][sim]\n",
    "                                mu_base = RegressionForest(honest=True, random_state=42, **mu_hyperparams)\n",
    "                            mse = fit_get_mse_s(train, test, mu_base)\n",
    "                            print('     MSE=', mse)\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Training set size: 5000\n",
      "---------------------------\n",
      "Starting simA\n",
      "\n",
      "simA/sample_1/T-learner:\n",
      "     MSE= 88.63872206863563\n",
      "simA/sample_1/S-learner:\n",
      "     MSE= 29.504\n",
      "\n",
      "simA/sample_2/T-learner:\n",
      "     MSE= 129.42625661000852\n",
      "simA/sample_2/S-learner:\n",
      "     MSE= 29.36192\n",
      "\n",
      "simA/sample_3/T-learner:\n",
      "     MSE= 95.17221816324512\n",
      "simA/sample_3/S-learner:\n",
      "     MSE= 29.50784\n",
      "---------------------------\n",
      "Starting simB\n",
      "\n",
      "simB/sample_1/T-learner:\n",
      "     MSE= 6775.965620493603\n",
      "simB/sample_1/S-learner:\n",
      "     MSE= 3662.7435690545954\n",
      "\n",
      "simB/sample_2/T-learner:\n",
      "     MSE= 5400.17845111733\n",
      "simB/sample_2/S-learner:\n",
      "     MSE= 2222.1920170910043\n",
      "\n",
      "simB/sample_3/T-learner:\n",
      "     MSE= 2883.7380421634894\n",
      "simB/sample_3/S-learner:\n",
      "     MSE= 1637.1524024289142\n",
      "---------------------------\n",
      "Training set size: 10000\n",
      "---------------------------\n",
      "Starting simA\n",
      "\n",
      "simA/sample_1/T-learner:\n",
      "     MSE= 38.3704882364476\n",
      "simA/sample_1/S-learner:\n",
      "     MSE= 29.30432\n",
      "\n",
      "simA/sample_2/T-learner:\n",
      "     MSE= 50.65774896220486\n",
      "simA/sample_2/S-learner:\n",
      "     MSE= 29.56608\n",
      "\n",
      "simA/sample_3/T-learner:\n",
      "     MSE= 64.24234072679285\n",
      "simA/sample_3/S-learner:\n",
      "     MSE= 29.50464\n",
      "---------------------------\n",
      "Starting simB\n",
      "\n",
      "simB/sample_1/T-learner:\n",
      "     MSE= 4228.635227835762\n",
      "simB/sample_1/S-learner:\n",
      "     MSE= 1948.385394433593\n",
      "\n",
      "simB/sample_2/T-learner:\n",
      "     MSE= 2588.2702182109547\n",
      "simB/sample_2/S-learner:\n",
      "     MSE= 1881.0365283121694\n",
      "\n",
      "simB/sample_3/T-learner:\n",
      "     MSE= 3974.078113425936\n",
      "simB/sample_3/S-learner:\n",
      "     MSE= 2754.3424072057614\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
