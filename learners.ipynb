{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.grf import RegressionForest\n",
    "from sklearn import clone\n",
    "import os, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class t_learner:\n",
    "\n",
    "    '''\n",
    "    From Rosenberg Slides:\n",
    "        µ^0(x) = E[Y (0) | X = x]\n",
    "        µ^1(x) = E[Y (1) | X = x]\n",
    "        τˆT (x) = µˆ1(x) − µˆ0(x)\n",
    "\n",
    "    authors: Alene Rhea and Tamar Novetsky, April 1 2021\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mu0_base, mu1_base):\n",
    "        # Make copies of initialized base learners\n",
    "        self.mu0_base = clone(mu0_base, safe=False)\n",
    "        self.mu1_base = clone(mu1_base, safe=False)\n",
    "\n",
    "    def fit(self, X, W, y):\n",
    "        # Call fit methods on each base learner\n",
    "        self.mu0_base.fit(X[W==0], y[W==0])\n",
    "        self.mu1_base.fit(X[W==1], y[W==1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y1_preds = self.mu1_base.predict(X)\n",
    "        y0_preds = self.mu0_base.predict(X)\n",
    "        tau_preds = y1_preds - y0_preds\n",
    "        return tau_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s_learner:\n",
    "\n",
    "    '''\n",
    "    From Rosenberg Slides:\n",
    "        µ^(x,w) = E[Y | X = x, W = w]\n",
    "        τˆS (x) = µˆ(x,1) − µˆ(x,0)\n",
    "\n",
    "    authors: Kelsey Markey and Lauren D'Arinzo, April 4 2021\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mu_base):\n",
    "        # Make copies of initialized base learner\n",
    "        self.mu_base = clone(mu_base, safe=False)\n",
    "\n",
    "    def fit(self, X_W, y):\n",
    "        # Call fit method\n",
    "        self.mu_base.fit(X_W, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_W_1 = X.copy(deep=True)\n",
    "        X_W_1[\"W\"] = pd.Series(np.ones(len(X_W_1)))\n",
    "        y1_preds = self.mu_base.predict(X_W_1)\n",
    "\n",
    "        X_W_0 = X.copy(deep=True)\n",
    "        X_W_0[\"W\"] = pd.Series(np.zeros(len(X_W_0)))\n",
    "        y0_preds = self.mu_base.predict(X_W_0)\n",
    "\n",
    "        tau_preds = y1_preds - y0_preds\n",
    "        return tau_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x_learner:\n",
    "\n",
    "    '''\n",
    "    From Rosenberg Slides:\n",
    "        µ^0(x) = E[Y (0) | X = x]\n",
    "        µ^1(x) = E[Y (1) | X = x]\n",
    "        τˆx (x) = g(x)t_0ˆ(x) − (1-g(x))t_1ˆ(x)\n",
    "\n",
    "    authors: Kelsey Markey and Lauren D'Arinzo, April 4 2021\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mu0_base, mu1_base, tau0_base, tau1_base):\n",
    "        # Make copies of initialized base learner\n",
    "        self.mu0_base = clone(mu0_base, safe=False)\n",
    "        self.mu1_base = clone(mu1_base, safe=False)\n",
    "        self.tau0_base = clone(tau0_base, safe=False)\n",
    "        self.tau1_base = clone(tau1_base, safe=False)\n",
    "\n",
    "    def fit(self, X, W, y, fit_g):\n",
    "        '''\n",
    "        fit_g : boolean indicator if g should be fit to training data. if false, g must be passed explicitlly to x_learner.predict()\n",
    "        '''\n",
    "        # Call fit method\n",
    "        self.mu0_base.fit(X[W==0], y[W==0])\n",
    "        self.mu1_base.fit(X[W==1], y[W==1])\n",
    "    \n",
    "        #Impute y0 for treated group using mu0\n",
    "        y0_treat=self.mu0_base.predict(X[W==1]).flatten()\n",
    "        imputed_TE_treatment = y[W==1] - y0_treat\n",
    "\n",
    "        #Impute y1 for control group using mu1\n",
    "        y1_control=self.mu1_base.predict(X[W==0]).flatten()\n",
    "        imputed_TE_control = y1_control - y[W==0] \n",
    "\n",
    "        #Fit tau0: CATE estimate fit to the control group\n",
    "        self.tau0_base.fit(X[W==0], imputed_TE_control)\n",
    "\n",
    "        #Fit tau1: CATE estimate fit to the treatment group\n",
    "        self.tau1_base.fit(X[W==1], imputed_TE_treatment)\n",
    "\n",
    "        if fit_g:\n",
    "            print('X Learner with g fitted')\n",
    "            #predict propensities from empirical data\n",
    "            self.g_fit = LogisticRegression(fit_intercept=True, max_iter=2000).fit(\n",
    "            X=X, y=W)\n",
    "        \n",
    "        else:\n",
    "            print('X Learner with true propensities')\n",
    "            self.g_fit = None\n",
    "\n",
    "\n",
    "    def predict(self, X, g):\n",
    "        '''\n",
    "        g : weight vector that should be length of the test set. can be passed as None if g was fit to data\n",
    "        '''\n",
    "        tau0_preds = self.tau0_base.predict(X).flatten()\n",
    "        tau1_preds = self.tau1_base.predict(X).flatten()\n",
    "    \n",
    "    \n",
    "        if self.g_fit == None:\n",
    "            tau_preds = (g.T * tau0_preds) + ((1-g).T*tau1_preds)\n",
    "        else:\n",
    "            g_preds = self.g_fit.predict_proba(X)[:, 1]\n",
    "            tau_preds = (g_preds.T * tau0_preds) + ((1-g_preds).T*tau1_preds)\n",
    "\n",
    "        # idea: allow g to be either be a vector or function?\n",
    "        # if function: think about sklearn inputs (.predict) vs lamba functions (g(x))\n",
    "        # if g_type == 'Function':\n",
    "        #    g_preds = g(X)\n",
    "        #    tau_preds = (g_preds.T * tau0_preds) + ((1-g_preds).T*tau1_preds)\n",
    "\n",
    "        return tau_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_get_mse_t(train, test, mu0_base, mu1_base):\n",
    "    '''\n",
    "    mu0_base: base learner that has already been initialized\n",
    "    mu1_base: base learner that has already been initialized\n",
    "    '''\n",
    "    #data preprocessing\n",
    "    X_train = train.drop(columns=['treatment', 'Y', 'tau'])\n",
    "    y_train = train['Y']\n",
    "    W_train = train['treatment']\n",
    "    X_test = test.drop(columns=['treatment', 'Y', 'tau'])\n",
    "\n",
    "    #initialize metalearner\n",
    "    T = t_learner(mu0_base=mu0_base, mu1_base=mu1_base)\n",
    "    T.fit(X=X_train, W=W_train, y=y_train)\n",
    "    \n",
    "    # Predict test-set CATEs\n",
    "    tau_preds = T.predict(X=X_test)\n",
    "\n",
    "    # Calculate MSE on test set\n",
    "    mse = np.mean((tau_preds - test.tau)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_get_mse_s(train, test, mu_base):\n",
    "    '''\n",
    "    mu: base learner that has already been initialized\n",
    "    '''\n",
    "    #data preprocessing\n",
    "    X_train = train.drop(columns=['treatment', 'Y', 'tau'])\n",
    "    y_train = train['Y']\n",
    "    W_train = train['treatment']\n",
    "    X_test = test.drop(columns=['treatment', 'Y', 'tau'])\n",
    "\n",
    "    #initialize metalearner\n",
    "    S = s_learner(mu_base=mu_base)\n",
    "    \n",
    "    #fit S-learner\n",
    "    X_W = pd.concat([X_train, W_train], axis=1)\n",
    "    S.fit(X_W=X_W, y=y_train)\n",
    "    \n",
    "    # Predict test-set CATEs\n",
    "    tau_preds = S.predict(X=X_test)\n",
    "\n",
    "    # Calculate MSE on test set\n",
    "    mse = np.mean((tau_preds - test.tau)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #hyperparameter dictionary (to be read in from file)\n",
    "    rf_t = {\"mu_0\": {\"simA\": {'n_estimators': 200, 'min_samples_split': 15}, \n",
    "                 \"simB\": {'n_estimators': 300, 'min_samples_split': 100}}, \n",
    "        \"mu_1\": {\"simA\" :{'n_estimators': 100, 'min_samples_split': 20}, \n",
    "                 \"simB\": {'n_estimators': 200, 'min_samples_split': 15}}\n",
    "       } \n",
    "    \n",
    "    \n",
    "    rf_s = {\"mu\": {\"simA\": {'n_estimators': 200, 'min_samples_split': 15}, \n",
    "                 \"simB\": {'n_estimators': 300, 'min_samples_split': 100}}, \n",
    "       } \n",
    "    \n",
    "    \n",
    "    \n",
    "    #rf_params = {'T': rf_T, 'S': rf_S, 'X': rf_X}\n",
    "    rf_params = {'T': rf_t, 'S': rf_s}\n",
    "    \n",
    "    #dictionary of all base learner combinations for each metalearner\n",
    "    meta_base_dict = {\"T\": [{'mu_0': 'rf', 'mu_1': 'rf'}], \"S\": [{'mu': 'rf'}]}\n",
    "    \n",
    "    # Set root directory\n",
    "    # base_repo_dir = pathlib.Path(os.path.realpath(__file__)).parents[0]\n",
    "    base_repo_dir = os.getcwd()\n",
    "    \n",
    "    for sim in ['simA', 'simB']:\n",
    "        for i in range(5):\n",
    "            train = pd.read_parquet(base_repo_dir + '/data/'  +sim+  '/samp'+str(i+1)+'_train.parquet')\n",
    "            test = pd.read_parquet(base_repo_dir + '/data/' +sim+ '/samp'+str(i+1)+'_test.parquet')\n",
    "            for metalearner in meta_base_dict.keys():\n",
    "                \n",
    "                if metalearner == 'T':\n",
    "                # below logic needs to be generalized for other metalearners\n",
    "                    for base_learner_dict in meta_base_dict[metalearner]:\n",
    "                        print(sim+',' ,metalearner+'-learner,', 'sample '+str(i+1))\n",
    "                        if base_learner_dict['mu_0'] == 'rf':\n",
    "                            mu0_hyperparams = rf_params[metalearner]['mu_0'][sim]\n",
    "                            mu0_base = RegressionForest(honest=True, random_state=42, **mu0_hyperparams)\n",
    "                        if base_learner_dict['mu_1'] == 'rf':\n",
    "                            mu1_hyperparams = rf_params[metalearner]['mu_1'][sim]\n",
    "                            mu1_base = RegressionForest(honest=True, random_state=42, **mu1_hyperparams)\n",
    "                        mse = fit_get_mse_t(train, test, mu0_base, mu1_base)\n",
    "                        print('     MSE:', mse)\n",
    "                        \n",
    "                if metalearner == 'S':\n",
    "                    for base_learner_dict in meta_base_dict[metalearner]:\n",
    "                        print(sim+',' ,metalearner+'-learner,', 'sample '+str(i+1))\n",
    "                        if base_learner_dict['mu'] == 'rf':\n",
    "                            mu_hyperparams = rf_params[metalearner]['mu'][sim]\n",
    "                            mu_base = RegressionForest(honest=True, random_state=42, **mu_hyperparams)\n",
    "                        mse = fit_get_mse_s(train, test, mu_base)\n",
    "                        print('     MSE:', mse)\n",
    "                        \n",
    "                    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simA, T-learner, sample 1\n",
      "     MSE: 45.059553114477886\n",
      "simA, S-learner, sample 1\n",
      "     MSE: 29.248\n",
      "simA, T-learner, sample 2\n",
      "     MSE: 88.4944375730485\n",
      "simA, S-learner, sample 2\n",
      "     MSE: 29.952\n",
      "simA, T-learner, sample 3\n",
      "     MSE: 76.32740435168\n",
      "simA, S-learner, sample 3\n",
      "     MSE: 28.992\n",
      "simA, T-learner, sample 4\n",
      "     MSE: 69.51870275103623\n",
      "simA, S-learner, sample 4\n",
      "     MSE: 31.552\n",
      "simA, T-learner, sample 5\n",
      "     MSE: 156.9701792218373\n",
      "simA, S-learner, sample 5\n",
      "     MSE: 30.72\n",
      "simB, T-learner, sample 1\n",
      "     MSE: 3237.5162109749594\n",
      "simB, S-learner, sample 1\n",
      "     MSE: 3346.9485038082075\n",
      "simB, T-learner, sample 2\n",
      "     MSE: 5317.564180434187\n",
      "simB, S-learner, sample 2\n",
      "     MSE: 3345.4609885086766\n",
      "simB, T-learner, sample 3\n",
      "     MSE: 2985.939686924113\n",
      "simB, S-learner, sample 3\n",
      "     MSE: 3344.213732969295\n",
      "simB, T-learner, sample 4\n",
      "     MSE: 4865.050048433415\n",
      "simB, S-learner, sample 4\n",
      "     MSE: 3348.007279500671\n",
      "simB, T-learner, sample 5\n",
      "     MSE: 2944.8534768512427\n",
      "simB, S-learner, sample 5\n",
      "     MSE: 3348.007279500671\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
